{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting Started: Airbnb Copenhagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment deals with the most recent Airbnb listings in Copenhagen. The data is collected from [Inside Airbnb](http://insideairbnb.com/copenhagen). Feel free to explore the website further in order to better understand the data. The data (*listings.csv*) has been collected as raw data and needs to be preprocessed.\n",
    "\n",
    "**Hand-in:** Hand in as a group in Itslearning in a **single**, well-organized and easy-to-read Jupyter Notebook. If your group consists of students from different classes, upload in **both** classes.\n",
    "\n",
    "1. First we need to remove all the redundant columns. Please keep the following 22 columns and remove all others:\n",
    "\n",
    "    id\\\n",
    "    name  \n",
    "    host_id  \n",
    "    host_name  \n",
    "    neighbourhood_cleansed  \n",
    "    latitude  \n",
    "    longitude  \n",
    "    room_type  \n",
    "    price  \n",
    "    minimum_nights  \n",
    "    number_of_reviews  \n",
    "    last_review  \n",
    "    review_scores_rating  \n",
    "    review_scores_accuracy  \n",
    "    review_scores_cleanliness  \n",
    "    review_scores_checkin  \n",
    "    review_scores_communication  \n",
    "    review_scores_location  \n",
    "    review_scores_value  \n",
    "    reviews_per_month  \n",
    "    calculated_host_listings_count  \n",
    "    availability_365\n",
    "\n",
    "\n",
    "\n",
    "2. Next we have to handle missing values. Remove all rows where `number_of_reviews = 0`. If there are still missing values, remove the rows that contain them so you have a data set with no missing values.\n",
    "\n",
    "3. Fix the `neighbourhood_cleansed` values (some are missing 'æ ø å'), and if necessary change the price to DKK.\n",
    "\n",
    "4. Create a fitting word cloud based on the `name` column. Feel free to remove non-descriptive stop words (e.g. since this is about Copenhagen, perhaps the word 'Copenhagen' is redundant).\n",
    "\n",
    "5. Since data science is so much fun, provide a word cloud of the names of the hosts, removing any names of non-persons. Does this more or less correspond with the distribution of names according to [Danmarks Statistik](https://www.dst.dk/da/Statistik/emner/borgere/navne/navne-i-hele-befolkningen)?\n",
    "\n",
    "6. Create a new column using bins of price. Use 11 bins, evenly distributed but with the last bin $> 10,000$.\n",
    "\n",
    "7. Using non-scaled versions of latitude and longitude, plot the listings data on a map. Use the newly created price bins as a color parameter. Also, create a plot (i.e. another plot) where you group the listings with regard to the neighbourhood.\n",
    "\n",
    "8. Create boxplots where you have the neighbourhood on the x-axis and price on the y-axis. What does this tell you about the listings in Copenhagen? Keep the x-axis as is and move different variables into the y-axis to see how things are distributed between the neighborhoods to create different plots (your choice).\n",
    "\n",
    "9. Create a bar chart of the hosts with the top ten most listings. Place host id on the x-axis and the count of listings on the y-axis.\n",
    "\n",
    "10. Do a descriptive analysis of the neighborhoods. Include information about room type in the analysis as well as one other self-chosen feature. The descriptive analysis should contain mean/average, mode, median, standard deviation/variance, minimum, maximum and quartiles.\n",
    "\n",
    "11. Supply a list of the top 10 highest rated listings and visualize them on a map.\n",
    "\n",
    "12. Now, use any preprocessing and feature engineering steps that you find relevant before proceeding (optional).\n",
    "\n",
    "13. Create another new column, where the price is divided into two categories: \"expensive\" listings defined by all listings with a price higher than the median price, and \"affordable\" listings defined by all listings with a price equal to or below the median price. You can encode the affordable listings as \"0\" and the expensive ones as \"1\". All listings should now have a classification indicating either expensive listings (1) or affordable listings (0).\n",
    "\n",
    "14. Based on self-chosen features, develop a Naïve Bayes and k-Nearest Neighbor model to determine whether a rental property should be classified as 0 or 1. Remember to divide your data into training data and test data. Comment on your findings.\n",
    "\n",
    "15. Try to come up with a final conclusion to the Airbnb-Copenhagen assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to remove all the redundant columns. Please keep the following 22 columns and remove all others\n",
    "\n",
    "\n",
    "Next we have to handle missing values. Remove all rows where `number_of_reviews = 0`. If there are still missing values,\n",
    "remove the rows that contain them so you have a data set with no missing values.\n",
    "\n",
    "Fix the `neighbourhood_cleansed` values (some are missing 'æ ø å'), and if necessary change the price to DKK.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('listings.csv')\n",
    "df = df[['id', 'name', 'host_id', 'host_name', 'neighbourhood_cleansed', 'latitude', 'longitude', 'room_type', 'price', 'minimum_nights', 'number_of_reviews', 'last_review', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']]\n",
    "df = df[df['number_of_reviews'] != 0]\n",
    "df = df.dropna()\n",
    "df['neighbourhood_cleansed'] = df['neighbourhood_cleansed'].str.replace('æ', 'ae')\n",
    "df['neighbourhood_cleansed'] = df['neighbourhood_cleansed'].str.replace('ø', 'oe')\n",
    "df['neighbourhood_cleansed'] = df['neighbourhood_cleansed'].str.replace('å', 'aa')\n",
    "df['price'] = df['price'].str.replace('$', '')\n",
    "df['price'] = df['price'].str.replace(',', '')\n",
    "df['price'] = df['price'].astype(float)\n",
    "df['price'] = df['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a fitting word cloud based on the `name` column. Feel free to remove non-descriptive stop words (e.g. since this is about Copenhagen, perhaps the word 'Copenhagen' is redundant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.add('copenhagen')\n",
    "stopwords.add('dtype')\n",
    "stopwords.add('og')\n",
    "stopwords.add('fra')\n",
    "stopwords.add('N')\n",
    "stopwords.add('ude')\n",
    "stopwords.add('Name')\n",
    "wordcloud = WordCloud(\n",
    "    background_color='white',\n",
    "    stopwords=stopwords,\n",
    "    max_words=200,\n",
    "    max_font_size=200,\n",
    "    random_state=42\n",
    ").generate(str(df['name']))\n",
    "fig = plt.figure(1)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since data science is so much fun, provide a word cloud of the names of the hosts, removing any names of non-persons. Does this more or less correspond with the distribution of names according to [Danmarks Statistik](https://www.dst.dk/da/Statistik/emner/borgere/navne/navne-i-hele-befolkningen)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "stopwords.add('object')\n",
    "stopwords.add('host_name')\n",
    "stopwords.add('dtype')\n",
    "stopwords.add('length')\n",
    "stopwords.add('Name')\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    background_color='white',\n",
    "    stopwords=stopwords,\n",
    "    max_words=200,\n",
    "    max_font_size=200,\n",
    "    random_state=42\n",
    ").generate(str(df['host_name']))\n",
    "fig = plt.figure(1)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Create a new column using bins of price. Use 11 bins, evenly distributed but with the last bin $> 10,000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = list(range(0, 10001, 1000)) + [float('inf')]\n",
    "bin_labels = ['0-1000', '1000-2000', '2000-3000', '3000-4000', '4000-5000', '5000-6000', '6000-7000', '7000-8000', '8000-9000', '9000-10000', '10000+']\n",
    "df['price_bins'] = pd.cut(df['price'], bins=bin_edges, labels=bin_labels)\n",
    "value_counts = df['price_bins'].value_counts().loc[bin_labels]\n",
    "value_counts.plot(kind='bar', figsize=(10, 6), title='Price distribution')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show  the distribution of ln(price)\n",
    "df['ln_price'] = np.log(df['price'])\n",
    "df['ln_price'].hist(bins=11)\n",
    "plt.xlabel('ln(price)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = list(range(0, 11, 1)) + [float('inf')]\n",
    "bin_labels = ['0-1', '1-2', '2-3', '3-4', '4-5', '5-6', '6-7','7-8','8-9','9-10','10+']\n",
    "df['ln_price_bins'] = pd.cut(df['ln_price'], bins=bin_edges, labels=bin_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using non-scaled versions of latitude and longitude, plot the listings data on a map. Use the newly created price bins as a color parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "df_map = df[['latitude', 'longitude', 'ln_price']].dropna().reset_index(drop=True)\n",
    "\n",
    "m = folium.Map(location=[55.6761, 12.5683], zoom_start=11)\n",
    "\n",
    "def gradient_rgb_color(price, min_price, max_price):\n",
    "    normalized = (price - min_price) / (max_price - min_price)\n",
    "    red = int(255 * normalized)\n",
    "    green = int(255 * (1 - normalized))\n",
    "    blue = 0\n",
    "    \n",
    "    return \"#{:02x}{:02x}{:02x}\".format(red, green, blue)\n",
    "\n",
    "\n",
    "min_price = df_map['ln_price'].min()\n",
    "max_price = df_map['ln_price'].max()\n",
    "\n",
    "for i in range(0, len(df_map)):\n",
    "    price = df_map.iloc[i]['ln_price']\n",
    "    color = gradient_rgb_color(price, min_price, max_price)\n",
    "    folium.Circle(\n",
    "        location=[df_map.iloc[i]['latitude'], df_map.iloc[i]['longitude']],\n",
    "        popup=df_map.iloc[i]['ln_price'],\n",
    "        radius=10,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_color=color\n",
    "    ).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, create a plot (i.e. another plot) where you group the listings with regard to the neighbourhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['neighbourhood_cleansed'].value_counts().plot(kind='bar', figsize=(10, 6), title='Neighbourhood distribution')\n",
    "plt.xlabel('Neighbourhood')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create boxplots where you have the neighbourhood on the x-axis and price on the y-axis. What does this tell you about the listings in Copenhagen? Keep the x-axis as is and move different variables into the y-axis to see how things are distributed between the neighborhoods to create different plots (your choice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "boxplot_neighbourhood_price = sns.boxplot(x='neighbourhood_cleansed', y='price', data=df)\n",
    "boxplot_neighbourhood_price.set_xticklabels(boxplot_neighbourhood_price.get_xticklabels(), rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bar chart of the hosts with the top ten most listings. Place host id on the x-axis and the count of listings on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot_host_id = sns.countplot(x='host_id', data=df, order=df['host_id'].value_counts().iloc[:10].index)\n",
    "barplot_host_id.set_xticklabels(barplot_host_id.get_xticklabels(), rotation=90)\n",
    "barplot_host_id.set(xlabel='Host ID', ylabel='Count of Listings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calculated_host_listings_count = df.sort_values(by=['calculated_host_listings_count'], ascending=False)\n",
    "df_calculated_host_listings_count = df_calculated_host_listings_count.drop_duplicates(subset='host_id', keep='first')\n",
    "df_calculated_host_listings_count = df_calculated_host_listings_count.head(10)\n",
    "df_calculated_host_listings_count = df_calculated_host_listings_count[['host_id', 'calculated_host_listings_count']]\n",
    "\n",
    "barplot_calculated_host_listings_count = sns.barplot(x='host_id', y='calculated_host_listings_count', data=df_calculated_host_listings_count,\n",
    "                                                    order=df_calculated_host_listings_count.sort_values(by='calculated_host_listings_count', ascending=False)['host_id'])\n",
    "barplot_calculated_host_listings_count.set_xticklabels(barplot_calculated_host_listings_count.get_xticklabels(), rotation=90)\n",
    "barplot_calculated_host_listings_count.set(xlabel='Host ID', ylabel='Count of Listings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a descriptive analysis of the neighborhoods. Include information about room type in the analysis as well as one other self-chosen feature. The descriptive analysis should contain mean/average, mode, median, standard deviation/variance, minimum, maximum and quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbourhood = df.groupby('neighbourhood_cleansed')\n",
    "neighbourhood['room_type'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbourhood['price'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbourhood['minimum_nights'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neighbourhood['price'].quantile([0.25, 0.5, 0.75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supply a list of the top 10 highest rated listings and visualize them on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top10_highest = df[['latitude', 'longitude', 'price', 'price_bins', 'review_scores_rating']].dropna().reset_index(drop=True).sort_values(by='review_scores_rating', ascending=False).head(10)\n",
    "m = folium.Map(location=[55.6761, 12.5683], zoom_start=11)\n",
    "marker_cluster = MarkerCluster().add_to(m)\n",
    "for _, row in df_top10_highest.iterrows():\n",
    "    color = categorize(row['price'])\n",
    "    folium.CircleMarker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        radius=5,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_color=color,\n",
    "        popup=f\"Price: {row['price']} - Bin: {row['price_bins']}\"\n",
    "    ).add_to(marker_cluster)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use any preprocessing and feature engineering steps that you find relevant before proceeding (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessing = df[['neighbourhood_cleansed', 'room_type', 'price', 'minimum_nights']]\n",
    "df_preprocessing = pd.concat([df_preprocessing, pd.get_dummies(df_preprocessing['room_type'])], axis=1)\n",
    "df_preprocessing = pd.concat([df_preprocessing, pd.get_dummies(df_preprocessing['neighbourhood_cleansed'])], axis=1)\n",
    "df_preprocessing = df_preprocessing.drop(['neighbourhood_cleansed', 'room_type'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create another new column, where the price is divided into two categories: \"expensive\" listings defined by all listings with a price higher than the median price, and \"affordable\" listings defined by all listings with a price equal to or below the median price. You can encode the affordable listings as \"0\" and the expensive ones as \"1\". All listings should now have a classification indicating either expensive listings (1) or affordable listings (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessing['expensive'] = np.where(df_preprocessing['price'] > df_preprocessing['price'].median(), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on self-chosen features, develop a Naïve Bayes and k-Nearest Neighbor model to determine whether a rental property should be classified as 0 or 1. Remember to divide your data into training data and test data. Comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_preprocessing['expensive']\n",
    "X = df_preprocessing.drop(['expensive'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)\n",
    "print('Accuracy score for Naive Bayes: ', accuracy_score(y_test, y_pred))\n",
    "print('AUC score for Naive Bayes: ', roc_auc_score(y_test, y_pred))\n",
    "scores = cross_val_score(gnb, X, y, cv=5, scoring='roc_auc')\n",
    "print('Cross validation scores for Naive Bayes: ', scores)\n",
    "print('Mean cross validation score for Naive Bayes: ', scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print('Accuracy score for KNN: ', accuracy_score(y_test, y_pred))\n",
    "print('AUC score for KNN: ', roc_auc_score(y_test, y_pred))\n",
    "scores = cross_val_score(knn, X, y, cv=5, scoring='roc_auc')\n",
    "print('Cross validation scores for KNN: ', scores)\n",
    "print('Mean cross validation score for KNN: ', scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try XGBoost classifier\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "print('Accuracy score for XGBoost: ', accuracy_score(y_test, y_pred))\n",
    "print('AUC score for XGBoost: ', roc_auc_score(y_test, y_pred))\n",
    "scores = cross_val_score(xgb_model, X, y, cv=5, scoring='roc_auc')\n",
    "print('Cross validation scores for XGBoost: ', scores)\n",
    "print('Mean cross validation score for XGBoost: ', scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to come up with a final conclusion to the Airbnb-Copenhagen assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
